<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Quarkus Superheroes: Managed services save the day</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/28/quarkus-superheroes-managed-services-save-day" /><author><name>Eric Deandrea</name></author><id>fc4ac9e3-f97b-4a22-b32d-bb1728224dcc</id><updated>2022-07-28T16:30:00Z</updated><published>2022-07-28T16:30:00Z</published><summary type="html">&lt;p&gt;Are you a developer building &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;? Do you struggle with developing and testing individual microservices that are part of a more extensive system? Would you rather focus on your applications and let something else manage the services they require?&lt;/p&gt; &lt;p&gt;This article introduces the &lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue"&gt;Quarkus Superheroes sample application&lt;/a&gt;, shows how to deploy it to the free &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, and then illustrates how &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; developers can modify the application to replace the backing services with fully managed services provided by &lt;a href="https://cloud.redhat.com/products/application-services"&gt;Red Hat OpenShift Application Services&lt;/a&gt;. The article might seem long, but running the steps should take only 15 to 20 minutes. There are many screenshots to help guide you on your way.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Here's what you will need if you want to follow along with the steps in this article:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A Red Hat account, which you need in order to create the managed &lt;a href="https://console.redhat.com/application-services/overview"&gt;Application Services&lt;/a&gt; on the &lt;a href="https://console.redhat.com"&gt;Red Hat Hybrid Cloud Console&lt;/a&gt; and access the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;. No credit card is required.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;oc&lt;/code&gt; &lt;a href="https://docs.openshift.com/container-platform/4.9/cli_reference/openshift_cli/getting-started-cli.html"&gt;Red Hat OpenShift command-line interface (CLI)&lt;/a&gt;, or &lt;a href="https://kubernetes.io/docs/tasks/tools/#kubectl"&gt;kubectl&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;A &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; development environment. This article uses the &lt;a href="https://developers.redhat.com/articles/2021/12/14/explore-java-17-language-features-quarkus"&gt;Java 17&lt;/a&gt; version of the application, but any of the other three versions (natively-compiled Java 11, JVM Java 11, or natively-compiled Java 17) would work just the same.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Quarkus Superheroes sample application&lt;/h2&gt; &lt;p&gt;&lt;a href="products/quarkus/overview"&gt;Quarkus&lt;/a&gt; has excellent &lt;a href="https://quarkus.io/guides"&gt;documentation&lt;/a&gt; and &lt;a href="https://github.com/quarkusio/quarkus-quickstarts"&gt;quickstarts&lt;/a&gt; to help you become familiar with various features in the Quarkus ecosystem. However, what was missing before was a fully implemented sample set of real-world applications that use these features, patterns, and best practices while also reflecting the problems Quarkus is trying to solve.&lt;/p&gt; &lt;p&gt;Released in February 2022, the &lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue"&gt;Quarkus Superheroes application&lt;/a&gt; consists of several microservices co-existing to form a more extensive system. Some microservices communicate synchronously via REST. Others are event-driven, producing and consuming events to and from &lt;a href="https://kafka.apache.org"&gt;Apache Kafka&lt;/a&gt;. Some microservices are &lt;a href="https://www.reactivemanifesto.org"&gt;reactive&lt;/a&gt;, while others are traditional.&lt;/p&gt; &lt;p&gt;Figure 1 shows the overall architecture of the application.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/arch_2.png?itok=jMAit2Kb" width="1440" height="1561" alt="The architecture of Quarkus Superheroes consists of elements of the game, plus services for a UI and statistics." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Overall architecture of the Quarkus Superheroes.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Detailed information about the application and its architecture can be found on the &lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue"&gt;quarkus.io blog&lt;/a&gt;. One of the &lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue/#requirements"&gt;main requirements&lt;/a&gt; when building the application was that it should be simple to deploy on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. Let's test that theory.&lt;/p&gt; &lt;h2&gt;Deploy the application on the Developer Sandbox for Red Hat OpenShift&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; provides you with a private &lt;a href="products/openshift"&gt;Red Hat OpenShift&lt;/a&gt; environment, free for use for 30 days, in a shared, multi-tenant OpenShift cluster that is preconfigured with a set of developer tools. Your private OpenShift environment includes two projects (namespaces) and a resource quota of 7GB RAM and 15GB storage. Your application's development and stage phases can be emulated using the two namespaces. All user &lt;code&gt;Pod&lt;/code&gt;s are automatically deleted 12 hours after being created.&lt;/p&gt; &lt;h3&gt;Log into the Developer Sandbox&lt;/h3&gt; &lt;p&gt;You can spin up and access your Developer Sandbox with your Red Hat account. &lt;a href="https://redhat-scholars.github.io/managed-kafka-service-registry-workshop/managed-kafka-service-registry-workshop/main/03-quarkus-app-with-kafka-service-registry.html#devsandboxaccess"&gt;Follow these instructions&lt;/a&gt; to log into your Developer Sandbox account. Don't worry if you don't yet have a Red Hat account. The instructions will guide you through how to create and verify one.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You need to follow only the six steps in the &lt;strong&gt;Get access to the Developer Sandbox&lt;/strong&gt; section of the instructions.&lt;/p&gt; &lt;p&gt;You can move to the next section once you are in the Developer Perspective of your sandbox.&lt;/p&gt; &lt;h3&gt;Connect your local machine to the Developer Sandbox&lt;/h3&gt; &lt;p&gt;Now you need to connect your local machine to your sandbox. Follow &lt;a href="https://developers.redhat.com/blog/2021/04/21/access-your-developer-sandbox-for-red-hat-openshift-from-the-command-line"&gt;these instructions&lt;/a&gt; to download the &lt;a href="https://docs.openshift.com/container-platform/4.9/cli_reference/openshift_cli/getting-started-cli.html"&gt;OpenShift CLI&lt;/a&gt; (if you don't already have it) and run &lt;code&gt;oc login&lt;/code&gt; with the token from your sandbox. Once done, your terminal should be set in the &lt;code&gt;&lt;your-username&gt;-dev&lt;/code&gt; project.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note: &lt;/strong&gt;If you already have a Developer Sandbox account and have existing workloads in your project, you might need to delete those before deploying the Quarkus Superheroes application. The Developer Sandbox limits the resources each user can deploy at a single time.&lt;/p&gt; &lt;h3&gt;Deploy Quarkus Superheroes&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;&lt;a href="https://github.com/quarkusio/quarkus-super-heroes/tree/main/deploy/k8s"&gt;deploy/k8s&lt;/a&gt;&lt;/code&gt; directory in the &lt;a href="https://github.com/quarkusio/quarkus-super-heroes"&gt;root of the repository&lt;/a&gt; contains Kubernetes descriptors for each of the four versions of the application: JVM 11, JVM 17, natively compiled with Java 11, and natively compiled with Java 17.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The Quarkus Superheroes repository contains Kubernetes descriptors for various flavors of Kubernetes: &lt;a href="https://www.openshift.com"&gt;OpenShift&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/deploying-to-kubernetes#deploying-to-minikube"&gt;minikube&lt;/a&gt;, &lt;a href="https://knative.dev"&gt;Knative&lt;/a&gt;, and "vanilla" &lt;a href="https://www.kubernetes.io"&gt;Kubernetes&lt;/a&gt;. The only real difference between the minikube and Kubernetes descriptors is that all the application &lt;code&gt;Service&lt;/code&gt;s in the minikube descriptors use &lt;code&gt;type: NodePort&lt;/code&gt;. A list of all the applications can be obtained simply by running &lt;code&gt;minikube service list&lt;/code&gt;. The Knative descriptors use &lt;a href="https://knative.dev/docs/serving"&gt;Knative Serving&lt;/a&gt; for each of the applications.&lt;/p&gt; &lt;p&gt;If you'd like, you can run &lt;code&gt;git clone&lt;/code&gt; to download the code from the &lt;a href="https://github.com/quarkusio/quarkus-super-heroes"&gt;Quarkus Superheroes GitHub repository&lt;/a&gt;. However, cloning isn't necessary because you can apply Kubernetes resources directly from remote locations.&lt;/p&gt; &lt;p&gt;Follow the following steps in your terminal to deploy the Java 17 version of the &lt;a href="https://quay.io/quarkus-super-heroes"&gt;application container images&lt;/a&gt;. Wait for each step to complete before proceeding with the next:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Deploy the application by executing: &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f https://raw.githubusercontent.com/quarkusio/quarkus-super-heroes/main/deploy/k8s/java17-openshift.yml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Deploy the &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt; monitoring service by executing: &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f https://raw.githubusercontent.com/quarkusio/quarkus-super-heroes/main/deploy/k8s/monitoring-openshift.yml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That's it—deploying the Superheroes is super simple! Once everything is deployed, your browser should look something like Figure 2.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/deployed.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/deployed.png?itok=24ZA2TGj" width="836" height="1160" alt="The topology for Quarkus Superheroes shows the relationships among the services." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Topology view for Quarkus Superheroes showing relationships among the services.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The application as deployed is &lt;em&gt;not&lt;/em&gt; considered production-ready. The databases, Prometheus instance, Kafka broker, and schema registry deployed are not highly available and do not use any Kubernetes operators for management or monitoring. They also use ephemeral storage.&lt;/p&gt; &lt;p&gt;Later in this article, we'll substitute a fully hosted and managed &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;Kafka service&lt;/a&gt; and &lt;a href="https://console.redhat.com/application-services/service-registry"&gt;schema registry service&lt;/a&gt; into the mix.&lt;/p&gt; &lt;h2&gt;Interacting with the application&lt;/h2&gt; &lt;p&gt;Open the event statistics user interface (UI) by clicking the icon in the upper right corner of the &lt;code&gt;event-statistics&lt;/code&gt; application, shown in Figure 3.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/opene.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/opene.png?itok=EZZUt00h" width="345" height="304" alt="The topology for the event statistics UI provides a button to open it." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Button for opening the event-statistics UI.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Once open, you should see the event-statistics UI shown in Figure 4.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure-4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/figure-4.png?itok=pPbIbTuU" width="913" height="246" alt="The event-statistics UI showing battle statistics" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The event-statistics UI showing battle statistics.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;There isn't anything here yet, but once we interact with the main application UI, there will be.&lt;/p&gt; &lt;p&gt;Similarly, open the Superheroes UI by clicking the icon in the upper right corner of the &lt;code&gt;ui-super-heroes&lt;/code&gt; application, shown in Figure 5.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/opens.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/opens.png?itok=gWclPICD" width="194" height="212" alt="The topology for the superheroes UI provides a button to open it." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Button for opening the Superheroes UI.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Once open, you should see the Superheroes UI, shown in Figure 6. Highlighted in green in Figure 6 are clickable areas such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Expand/collapse the list of powers a hero or villain has&lt;/li&gt; &lt;li&gt;Randomly select a new hero and villain&lt;/li&gt; &lt;li&gt;Perform a battle&lt;/li&gt; &lt;/ul&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; You will most likely see different fighters than in the screenshot. They are randomly chosen.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/super.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/super.png?itok=-waXJzpI" width="600" height="490" alt="The Superheroes UI shows a villain and hero, randomly chosen." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: The Superheroes UI shows a villain and hero, randomly chosen.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Go ahead and perform a few battles, both with the same fighters and with new fighters. Once you've completed a few battles, note that a chronological list of battles is now displayed in the table on the screen.&lt;/p&gt; &lt;p&gt;You can also switch your browser tab back over to the event statistics UI. The slider in the event statistics UI should have moved one way or another or stayed in the middle if there were equal wins. There should also be a list of the top ten winners and the number of wins for each. Figure 7 shows an example.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/event-statistics-ui.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/event-statistics-ui.png?itok=FgsRS-EX" width="908" height="293" alt="event-statistics UI after a few battles." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: event-statistics UI after a few battles.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Messages in &lt;a href="https://quarkus.io/guides/kafka-schema-registry-avro"&gt;Apache Avro&lt;/a&gt; format arrive from &lt;a href="https://kafka.apache.org"&gt;Apache Kafka&lt;/a&gt; from the &lt;code&gt;rest-fights&lt;/code&gt; service to the &lt;code&gt;event-statistics&lt;/code&gt; service. The schema for these messages is registered in an &lt;a href="https://www.apicur.io/registry"&gt;Apicurio Schema Registry&lt;/a&gt;. You can open the Apicurio Schema Registry by clicking the icon in the upper right corner of the &lt;code&gt;apicurio&lt;/code&gt; application, shown in Figure 8.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/opena.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/opena.png?itok=2HgygL5z" width="182" height="167" alt="The topology for the Apicurio UI provides a button to open it." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 8: Button for opening the Apicurio UI.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Once open, you should see the Apicurio UI, shown in Figure 9.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/apicurio.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/apicurio.png?itok=oqVaIUE-" width="581" height="347" alt="The Apicurio service registry instance contains a Fight instance." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 9: The Apicurio Service Registry instance containing a Fight schema.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Click the &lt;code&gt;Fight (fights-value)&lt;/code&gt; link to see all the details of the schema, including the Avro source of the schema itself.&lt;/p&gt; &lt;h2&gt;Creating managed services&lt;/h2&gt; &lt;p&gt;We already mentioned that the current setup is not production-ready. Furthermore, if the Kafka or Schema Registry &lt;code&gt;Pod&lt;/code&gt;s restart, all the data is lost. One way to fix this is to use fully hosted and managed Kafka and Schema Registry services.&lt;/p&gt; &lt;p&gt;Red Hat provides managed cloud services, known as the &lt;a href="https://console.redhat.com/application-services/overview"&gt;Red Hat OpenShift Application Services&lt;/a&gt;. We will use a free trial for two of these services, &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-streams-for-apache-kafka"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-service-registry"&gt;Red Hat OpenShift Service Registry&lt;/a&gt;, to provide a set of production-ready services for our application.&lt;/p&gt; &lt;h3&gt;&lt;a id="CreateServiceAccounts" name="CreateServiceAccounts"&gt;&lt;/a&gt;Creating service accounts&lt;/h3&gt; &lt;p&gt;The first thing you need to do is create a few service accounts to be used by your applications. Two applications communicate with Kafka in this architecture: The &lt;code&gt;event-statistics&lt;/code&gt; and &lt;code&gt;rest-fights&lt;/code&gt; services. Each service needs an individual service account for communicating with the managed services.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; In some instances, it might make sense for each service to have an individual service account for each managed service. In this article, we decided to keep things simple, so each service has a single service account for both managed services.&lt;/p&gt; &lt;p&gt;Follow these steps to create the two service accounts:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open a new browser tab to the &lt;a href="https://console.redhat.com/application-services"&gt;Red Hat OpenShift Application Services dashboard&lt;/a&gt;. The dashboard uses the Red Hat account with which you logged into the sandbox. You might need to accept some additional terms and conditions if this is the first time you've visited the dashboard.&lt;/li&gt; &lt;li&gt;On the left-hand navigation, select &lt;strong&gt;Service Accounts&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;In the middle of the screen, click &lt;strong&gt;Create service account&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Short description&lt;/strong&gt; field, enter &lt;code&gt;event-statistics&lt;/code&gt; and click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;Credentials successfully generated&lt;/strong&gt; screen is now shown. Save the &lt;strong&gt;Client ID&lt;/strong&gt; and &lt;strong&gt;Client secret&lt;/strong&gt; somewhere in a safe place. You will need them later, and they cannot be viewed again. These are the ID and secret for the &lt;code&gt;event-statistics&lt;/code&gt; service.&lt;/li&gt; &lt;li&gt;Once copied, check the &lt;strong&gt;I have copied the client ID and secret box&lt;/strong&gt; and click &lt;strong&gt;Close&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;Service Accounts&lt;/strong&gt; screen will be shown. Click &lt;strong&gt;Create service account&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Repeat steps 4-6, but use &lt;code&gt;rest-fights&lt;/code&gt; as the &lt;strong&gt;Short Description&lt;/strong&gt; and copy the generated ID and secret for the &lt;code&gt;rest-fights&lt;/code&gt; service somewhere safe.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Create the Service Registry service&lt;/h3&gt; &lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-service-registry"&gt;Red Hat OpenShift Service Registry&lt;/a&gt; is based on the open-source &lt;a href="https://www.apicur.io/registry/"&gt;Apicurio Registry project&lt;/a&gt;. The service provides a highly available service registry instance that is secure and compatible with the &lt;a href="https://docs.confluent.io/platform/current/schema-registry/develop/api.html"&gt;Confluent Schema Registry API&lt;/a&gt; and &lt;a href="https://github.com/cloudevents/spec/blob/main/schemaregistry/spec.md"&gt;CNCF Schema Registry API&lt;/a&gt;. OpenShift Service Registry is also a perfect companion service for applications that use &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/red-hat-openshift-api-management/overview"&gt;Red Hat OpenShift API Management&lt;/a&gt;.&lt;/p&gt; &lt;h4&gt;Create the Service Registry instance&lt;/h4&gt; &lt;p&gt;Follow these steps to create a Service Registry instance and give the service accounts the proper access:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;From the &lt;a href="https://console.redhat.com/application-services"&gt;Red Hat OpenShift Application Services dashboard&lt;/a&gt;, expand the &lt;strong&gt;Service Registry&lt;/strong&gt; entry on the left-side menu and click the &lt;strong&gt;Service Registry Instances&lt;/strong&gt; link.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create Service Registry Instance&lt;/strong&gt; button in the middle of the screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Name&lt;/strong&gt; field, enter &lt;code&gt;quarkus-superheroes&lt;/code&gt; and click &lt;strong&gt;Create&lt;/strong&gt;. &lt;ul&gt; &lt;li&gt;It might take a minute or so for the instance to create. The &lt;strong&gt;Status&lt;/strong&gt; column will display a green checkmark to indicate when the instance is ready for use, as shown in Figure 10.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/qs.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/qs.png?itok=0Y432kTj" width="1440" height="430" alt="The service registry shows that the quarkus-superheroes instance is ready." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 10: The Apicurio Service Registry showing the quarkus-superheroes instance is ready.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h4&gt;Assign service account access to the Service Registry&lt;/h4&gt; &lt;p&gt;Now follow these steps to give the two service accounts access to the instance:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Click the quarkus-superheroes instance.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Access&lt;/strong&gt; tab at the top.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Grant access&lt;/strong&gt; button in the middle of the screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Grant access&lt;/strong&gt; dialog that pops up, choose the service account for the &lt;code&gt;event-statistics&lt;/code&gt; service, choose the &lt;strong&gt;Manager&lt;/strong&gt; role, and click &lt;strong&gt;Save&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Repeat steps 3 and 4 to grant the &lt;strong&gt;Manager&lt;/strong&gt; role to the &lt;code&gt;rest-fights&lt;/code&gt; service account.&lt;/li&gt; &lt;/ol&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; We are giving the service accounts the &lt;strong&gt;Manager&lt;/strong&gt; role instead of the &lt;strong&gt;Viewer&lt;/strong&gt; role because both the &lt;code&gt;rest-fights&lt;/code&gt; and &lt;code&gt;event-statistics&lt;/code&gt; services publish the schema to the registry if it does not already exist.&lt;/p&gt; &lt;h4&gt;&lt;a id="GetServiceRegistryConnectionDetails" name="GetServiceRegistryConnectionDetails"&gt;&lt;/a&gt;Get Service Registry connection details&lt;/h4&gt; &lt;p&gt;Now follow these steps to get the connection details for the Service Registry instance:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Click the &lt;strong&gt;Service Registry Instances&lt;/strong&gt; link on the left-side menu to return to the list of Service Registry instances.&lt;/li&gt; &lt;li&gt;Click the menu on the right of the &lt;code&gt;quarkus-superheros&lt;/code&gt; instance and select &lt;strong&gt;Connection&lt;/strong&gt;, as shown in Figure 11. The screen displays the client connection information for the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/connection_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/connection_0.png?itok=UKHSCSm3" width="600" height="185" alt="The line for an instance includes a Connection button at the right." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 11: Get the Service Registry instance connection info.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Copy to clipboard&lt;/strong&gt; buttons for the &lt;strong&gt;Core Registry API&lt;/strong&gt; and the &lt;strong&gt;Token endpoint URL&lt;/strong&gt; fields as shown in Figure 12. Save these URLs in a safe place—they will be needed later when you configure the applications. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/buttons.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/buttons.png?itok=6mfydiBn" width="600" height="837" alt="The web page for an instance includes buttons for Core Registry API and Token endpoint URL." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 12: The Service Registry instance connection details.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;OpenShift Service Registry provides the following APIs to connect applications to the service:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;strong&gt;Core Registry API&lt;/strong&gt; is the most powerful and works with Apicurio client libraries. This is the endpoint that the applications will connect to.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;Schema Registry compatibility API&lt;/strong&gt; provides compatibility with the Confluent Schema Registry API.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;CNCF Schema Registry API&lt;/strong&gt; provides compatibility with the CNCF specification.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Creating an Apache Kafka instance&lt;/h3&gt; &lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-streams-for-apache-kafka"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; is a managed cloud service that provides a streamlined developer experience for building, deploying, and scaling new &lt;a href="https://www.redhat.com/en/topics/cloud-native-apps"&gt;cloud-native applications&lt;/a&gt; or modernizing existing systems.&lt;/p&gt; &lt;h4&gt;Create a Kafka instance&lt;/h4&gt; &lt;p&gt;Follow these steps to create a Kafka instance:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;From the &lt;a href="https://console.redhat.com/application-services"&gt;Red Hat OpenShift Application Services dashboard&lt;/a&gt;, expand the &lt;strong&gt;Streams for Apache Kafka&lt;/strong&gt; entry on the left-side menu, and click &lt;strong&gt;Kafka Instances&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create Kafka Instance&lt;/strong&gt; button in the middle of the screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Name&lt;/strong&gt; field, enter &lt;code&gt;quarkus-superheroes&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Cloud region&lt;/strong&gt; field, select &lt;strong&gt;US East, N. Virginia&lt;/strong&gt;. Trial instances are not available in the EU region.&lt;/li&gt; &lt;li&gt; &lt;p&gt;Click the &lt;strong&gt;Create Instance&lt;/strong&gt; button.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; There is limited capacity in the service's free tier. If you get a message saying "Something went wrong or There was a problem processing the request," try again later.&lt;/p&gt; &lt;/li&gt; &lt;li&gt;It might take a few minutes for the instance to create. The &lt;strong&gt;Status&lt;/strong&gt; column will display a green checkmark to indicate when the instance is ready for use, as shown in Figure 13.&lt;/li&gt; &lt;/ol&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ready.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ready.png?itok=4jAS6i4O" width="1440" height="472" alt="Kafka Instances page shows when an instance is ready in the Status field." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 13: The Kafka instances showing the quarkus-superheroes instance is ready.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h4&gt;Create a Kafka topic&lt;/h4&gt; &lt;p&gt;Now follow these steps to create a topic within the instance:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Click the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Topics&lt;/strong&gt; item on the top of the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance dashboard.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create topic&lt;/strong&gt; button in the middle of the screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Topic name&lt;/strong&gt; field, enter &lt;code&gt;fights&lt;/code&gt;, then click the &lt;strong&gt;Next&lt;/strong&gt; button at the bottom of the screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Partitions&lt;/strong&gt; field, leave the selection of 1 in place and click the &lt;strong&gt;Next&lt;/strong&gt; button at the bottom of the screen.&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;Message retention&lt;/strong&gt; screen, leave the defaults in place and click the &lt;strong&gt;Next&lt;/strong&gt; button at the bottom of the screen.&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;Replicas&lt;/strong&gt; screen, leave the defaults in place and click the &lt;strong&gt;Finish&lt;/strong&gt; button at the bottom of the screen. The topic will be created.&lt;/li&gt; &lt;/ol&gt; &lt;h4&gt;Assign service account access to Kafka&lt;/h4&gt; &lt;p&gt;Now follow these steps to assign proper access to the topic for the two service accounts:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Click the &lt;strong&gt;Access&lt;/strong&gt; item on the top of the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance dashboard.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Manage access&lt;/strong&gt; button at the top of the &lt;strong&gt;Access&lt;/strong&gt; screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Manage access&lt;/strong&gt; pop-up dialog that appears, select the &lt;code&gt;event-statistics&lt;/code&gt; service account from the &lt;strong&gt;Account&lt;/strong&gt; dropdown, then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the down arrow next to the &lt;strong&gt;Add permission&lt;/strong&gt; button at the bottom of the pop-up dialog. Select &lt;strong&gt;Consume from a topic&lt;/strong&gt;, as shown in Figure 14, then click the &lt;strong&gt;Save&lt;/strong&gt; button at the bottom of the dialog. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/consume.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/consume.png?itok=LES3Fcnf" width="600" height="535" alt="The Manage Access screen lets a service consume from a Kafka topic." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 14: Access management for consuming messages from a Kafka topic.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Manage access&lt;/strong&gt; button at the top of the &lt;strong&gt;Access&lt;/strong&gt; screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Manage access&lt;/strong&gt; pop-up dialog that appears, select the &lt;code&gt;rest-fights&lt;/code&gt; service account from the &lt;strong&gt;Account&lt;/strong&gt; dropdown, then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the down arrow next to the &lt;strong&gt;Add permission&lt;/strong&gt; button at the bottom of the pop-up dialog. Select &lt;strong&gt;Produce to a topic&lt;/strong&gt;, as shown in Figure 15, then click the &lt;strong&gt;Save&lt;/strong&gt; button at the bottom of the dialog. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/produce.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/produce.png?itok=zdPHNZrv" width="600" height="506" alt="The Manage Access screen lets a service produce to a Kafka topic." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 15: Access management for producing messages to a Kafka topic.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Figure 16 shows the &lt;strong&gt;Access&lt;/strong&gt; tab containing all the Kafka permissions.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/permissions_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/permissions_0.png?itok=nxyQV5mo" width="1371" height="843" alt="The Manage Access screen shows all Kafka permissions." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 16: Access management for all Kafka permissions.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h4&gt;&lt;a id="GetKafkaConnectionDetails" name="GetKafkaConnectionDetails"&gt;&lt;/a&gt;Get Kafka connection details&lt;/h4&gt; &lt;p&gt;Now follow these steps to get the connection details for the Kafka instance:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Click the &lt;strong&gt;Kafka Instances&lt;/strong&gt; link on the left-side menu to return to the list of Kafka instances.&lt;/li&gt; &lt;li&gt;Click the menu on the right of the &lt;code&gt;quarkus-superheros&lt;/code&gt; instance and select &lt;strong&gt;Connection&lt;/strong&gt;, as shown in Figure 17. This displays the client connection information for the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/info.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/info.png?itok=fv-1z74r" width="600" height="204" alt="The line for an instance includes a Connection button that can display client connection information." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 17: Get the Kafka instance connection info.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Copy to clipboard&lt;/strong&gt; buttons for the &lt;strong&gt;Bootstrap server&lt;/strong&gt; and the &lt;strong&gt;Token endpoint URL&lt;/strong&gt; fields, as shown in Figure 18. Save these URLs in a safe place, because they will be needed later when you configure the applications.&lt;/li&gt; &lt;/ol&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;strong&gt;Token endpoint URL&lt;/strong&gt; is most likely the same as the service registry &lt;strong&gt;Token endpoint URL&lt;/strong&gt;. Both services use the same identity provider.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/boot.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/boot.png?itok=BrmSAHja" width="676" height="887" alt="The web page for an instance includes buttons for Bootstrap server and Token endpoint URL." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 18: The Kafka instance connection details.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Connect the application to managed services&lt;/h2&gt; &lt;p&gt;Now, let's connect our applications to the managed services we've created. First, let's get a little disruptive by deleting the existing Apicurio Schema Registry and Kafka workloads.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Return to the Topology view of the Developer Sandbox, find the &lt;code&gt;apicurio&lt;/code&gt; workload, right-click it, and select &lt;strong&gt;Delete Deployment&lt;/strong&gt; as shown in Figure 19. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/delete.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/delete.png?itok=gqNj1lig" width="340" height="567" alt="The topology for an instance lets you delete the deployment." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 19: Delete Apicurio Deployment.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt;When the &lt;strong&gt;Delete Deployment&lt;/strong&gt; confirmation pops up, check the &lt;strong&gt;Delete dependent objects of this resource&lt;/strong&gt; box and click &lt;strong&gt;Delete&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Repeat steps 1 and 2 to delete the &lt;code&gt;fights-kafka&lt;/code&gt; workload as well.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Deleting these workloads might cause some of the other workloads to turn red or show errors in their logs, which is expected. Let's now fix the applications by connecting them to the managed services.&lt;/p&gt; &lt;p&gt;Every application has a corresponding &lt;code&gt;ConfigMap&lt;/code&gt; and &lt;code&gt;Secret&lt;/code&gt; containing its configuration and credentials. These two places in the &lt;code&gt;event-statistics&lt;/code&gt; and &lt;code&gt;rest-fights&lt;/code&gt; services need to be updated with the new configuration.&lt;/p&gt; &lt;h3&gt;Connect the event-statistics service&lt;/h3&gt; &lt;p&gt;Follow these steps to connect the &lt;code&gt;event-statistics&lt;/code&gt; service to the managed services:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Click the &lt;strong&gt;ConfigMaps&lt;/strong&gt; link on the left-hand navigation of the sandbox.&lt;/li&gt; &lt;li&gt;Find and click the &lt;code&gt;event-statistics-config&lt;/code&gt; item.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;YAML&lt;/strong&gt; tab at the top of the &lt;code&gt;event-statistics-config&lt;/code&gt; screen.&lt;/li&gt; &lt;li&gt;In the &lt;code&gt;data&lt;/code&gt; section near the bottom, perform the following: &lt;ul&gt; &lt;li&gt;Replace the value of &lt;code&gt;kafka.bootstrap.servers&lt;/code&gt; with the &lt;strong&gt;Bootstrap server&lt;/strong&gt; value you saved earlier from the &lt;a href="#GetKafkaConnectionDetails"&gt;Getting Kafka connection details&lt;/a&gt; section.&lt;/li&gt; &lt;li&gt;Replace the value of &lt;code&gt;mp.messaging.connector.smallrye-kafka.apicurio.registry.url&lt;/code&gt; with the &lt;strong&gt;Core Registry API&lt;/strong&gt; value you saved earlier from the &lt;a href="#GetServiceRegistryConnectionDetails"&gt;Getting service registry connection details&lt;/a&gt; section.&lt;/li&gt; &lt;li&gt; &lt;p&gt;Add the following new key/value combinations:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;mp.messaging.connector.smallrye-kafka.apicurio.auth.service.token.endpoint: &lt;Token endpoint URL&gt; mp.messaging.connector.smallrye-kafka.sasl.jaas.config: org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required oauth.client.id="${CLIENT_ID}" oauth.client.secret="${CLIENT_SECRET}" oauth.token.endpoint.uri="&lt;Token endpoint URL&gt;" ; mp.messaging.connector.smallrye-kafka.security.protocol: SASL_SSL mp.messaging.connector.smallrye-kafka.sasl.mechanism: OAUTHBEARER mp.messaging.connector.smallrye-kafka.sasl.login.callback.handler.class: io.strimzi.kafka.oauth.client.JaasClientOauthLoginCallbackHandler mp.messaging.connector.smallrye-kafka.apicurio.auth.client.id: ${CLIENT_ID} mp.messaging.connector.smallrye-kafka.apicurio.auth.client.secret: ${CLIENT_SECRET}&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Replace &lt;code&gt;&lt;Token endpoint URL&gt;&lt;/code&gt; with the &lt;strong&gt;Token endpoint URL&lt;/strong&gt; value you saved earlier from either the &lt;a href="#GetServiceRegistryConnectionDetails"&gt;Getting service registry connection details&lt;/a&gt; or &lt;a href="#GetKafkaConnectionDetails"&gt;Getting Kafka connection details&lt;/a&gt; section. The value should be the same in both places.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When complete, the data section in your &lt;code&gt;event-statistics-config&lt;/code&gt; &lt;code&gt;ConfigMap&lt;/code&gt; should look something like Figure 20.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/configmap_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/configmap_0.png?itok=Sxy29CwG" width="600" height="97" alt="The YAML in the event-statistics-config ConfigMap is complete." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 20: The event-statistics-config ConfigMap YAML contents.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Save&lt;/strong&gt; at the bottom of the screen.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Secrets&lt;/strong&gt; link on the left-hand navigation of the sandbox.&lt;/li&gt; &lt;li&gt;Find and click the &lt;code&gt;event-statistics-config-creds&lt;/code&gt; item.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;YAML&lt;/strong&gt; tab at the top of the &lt;code&gt;event-statistics-config-creds&lt;/code&gt; screen.&lt;/li&gt; &lt;li&gt;Add a &lt;code&gt;stringData&lt;/code&gt; element at the bottom and add the following key/value pairs, as shown in Figure 21: &lt;pre&gt; &lt;code&gt;CLIENT_ID: &lt;event-statistics-client-id&gt; CLIENT_SECRET: &lt;event-statistics-client-secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Replace &lt;code&gt;&lt;event-statistics-client-id&gt;&lt;/code&gt; with the client ID for the &lt;code&gt;event-statistics&lt;/code&gt; service you created in the &lt;a href="#CreateServiceAccounts"&gt;Creating service accounts&lt;/a&gt; section.&lt;/li&gt; &lt;li&gt;Replace &lt;code&gt;&lt;event-statistics-client-secret&gt;&lt;/code&gt; with the client secret for the &lt;code&gt;event-statistics&lt;/code&gt; service you created in the &lt;a href="#CreateServiceAccounts"&gt;Creating service accounts&lt;/a&gt; section. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/creds.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/creds.png?itok=qHPrpmXk" width="451" height="57" alt="The YAML for the event-statistics-config-creds shows CLIENT_ID and CLIENT_SECRET." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 21: The event-statistics-config-creds Secret YAML contents.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Save&lt;/strong&gt; at the bottom of the screen.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Topology&lt;/strong&gt; link on the left-hand navigation of the sandbox.&lt;/li&gt; &lt;li&gt;Find and right-click the &lt;code&gt;event-statistics&lt;/code&gt; workload, then select &lt;strong&gt;Start rollout&lt;/strong&gt; as shown in Figure 22. A new instance of the &lt;code&gt;event-statistics&lt;/code&gt; workload will be rolled out with the updated configuration. The rollout should only take a minute or less.&lt;/li&gt; &lt;/ol&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/roll.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/roll.png?itok=tw-07o1x" width="460" height="656" alt="The topology for event-statistics allows you to start the rollout." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 22: Start a rollout of the event-statistics service.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Move on and connect the &lt;code&gt;rest-fights&lt;/code&gt; service while the &lt;code&gt;event-statistics&lt;/code&gt; is rolling out.&lt;/p&gt; &lt;h3&gt;Connect the rest-fights service&lt;/h3&gt; &lt;p&gt;Next, follow these steps to connect the &lt;code&gt;rest-fights&lt;/code&gt; service to the managed services, which are nearly identical to the previous steps connecting the &lt;code&gt;event-statistics&lt;/code&gt; service:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Click the &lt;strong&gt;ConfigMaps&lt;/strong&gt; link on the left-hand navigation of the sandbox.&lt;/li&gt; &lt;li&gt;Find and click the &lt;code&gt;rest-fights-config&lt;/code&gt; item.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;YAML&lt;/strong&gt; tab at the top of the &lt;code&gt;rest-fights-config&lt;/code&gt; screen.&lt;/li&gt; &lt;li&gt;In the &lt;code&gt;data&lt;/code&gt; section toward the bottom, perform the following: &lt;ul&gt; &lt;li&gt;Replace the value of &lt;code&gt;kafka.bootstrap.servers&lt;/code&gt; with the &lt;strong&gt;Bootstrap server&lt;/strong&gt; value you saved earlier from the &lt;a href="#GetKafkaConnectionDetails"&gt;Get Kafka connection details&lt;/a&gt; section.&lt;/li&gt; &lt;li&gt;Replace the value of &lt;code&gt;mp.messaging.connector.smallrye-kafka.apicurio.registry.url&lt;/code&gt; with the &lt;strong&gt;Core Registry API&lt;/strong&gt; value you saved earlier from the &lt;a href="#GetServiceRegistryConnectionDetails"&gt;Get service registry connection details&lt;/a&gt; section.&lt;/li&gt; &lt;li&gt; &lt;p&gt;Add the following new key/value combinations:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;mp.messaging.connector.smallrye-kafka.apicurio.auth.service.token.endpoint: &lt;Token endpoint URL&gt; mp.messaging.connector.smallrye-kafka.sasl.jaas.config: org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required oauth.client.id="${CLIENT_ID}" oauth.client.secret="${CLIENT_SECRET}" oauth.token.endpoint.uri="&lt;Token endpoint URL&gt;" ; mp.messaging.connector.smallrye-kafka.security.protocol: SASL_SSL mp.messaging.connector.smallrye-kafka.sasl.mechanism: OAUTHBEARER mp.messaging.connector.smallrye-kafka.sasl.login.callback.handler.class: io.strimzi.kafka.oauth.client.JaasClientOauthLoginCallbackHandler mp.messaging.connector.smallrye-kafka.apicurio.auth.client.id: ${CLIENT_ID} mp.messaging.connector.smallrye-kafka.apicurio.auth.client.secret: ${CLIENT_SECRET}&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Replace &lt;code&gt;&lt;Token endpoint URL&gt;&lt;/code&gt; with the &lt;strong&gt;Token endpoint URL&lt;/strong&gt; value you saved earlier from either the &lt;a href="#GetServiceRegistryConnectionDetails"&gt;Get service registry connection details&lt;/a&gt; or &lt;a href="#GetKafkaConnectionDetails"&gt;Get Kafka connection details&lt;/a&gt; section. The value should be the same in both.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When complete, the data section in your &lt;code&gt;rest-fights-config&lt;/code&gt; &lt;code&gt;ConfigMap&lt;/code&gt; should look something like Figure 23.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rest.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rest.png?itok=0OcsKLsv" width="600" height="145" alt="The YAML in the rest-fights-config ConfigMap is complete." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 23: The rest-fights-config ConfigMap YAML contents.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Save&lt;/strong&gt; at the bottom of the screen.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Secrets&lt;/strong&gt; link on the left-hand navigation of the sandbox.&lt;/li&gt; &lt;li&gt;Find and click the &lt;code&gt;rest-fights-config-creds&lt;/code&gt; item.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;YAML&lt;/strong&gt; tab at the top of the &lt;code&gt;rest-fights-config-creds&lt;/code&gt; screen.&lt;/li&gt; &lt;li&gt;Add a &lt;code&gt;stringData&lt;/code&gt; element at the bottom and add the following key/value pairs, as shown in Figure 24: &lt;pre&gt; &lt;code&gt;CLIENT_ID: &lt;rest-fights-client-id&gt; CLIENT_SECRET: &lt;rest-fights-client-secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;ul class="Indent1"&gt; &lt;li&gt;Replace &lt;code&gt;&lt;rest-fights-client-id&gt;&lt;/code&gt; with the client ID for the &lt;code&gt;rest-fights&lt;/code&gt; service you created in the &lt;a href="#CreateServiceAccounts"&gt;Creating service accounts&lt;/a&gt; section.&lt;/li&gt; &lt;li&gt;Replace &lt;code&gt;&lt;rest-fights-client-secret&gt;&lt;/code&gt; with the client secret for the &lt;code&gt;rest-fights&lt;/code&gt; service you created in the &lt;a href="#CreateServiceAccounts"&gt;Creating service accounts&lt;/a&gt; section. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/yc.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/yc.png?itok=tbF2E7-G" width="456" height="63" alt="The YAML for the rest-fights-config-creds shows CLIENT_ID and CLIENT_SECRET." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 24: The rest-fights-config-creds Secret YAML contents.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Save&lt;/strong&gt; at the bottom of the screen.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Topology&lt;/strong&gt; link on the left-hand navigation of the sandbox.&lt;/li&gt; &lt;li&gt;Find and right-click the &lt;code&gt;rest-fights&lt;/code&gt; workload, then select &lt;strong&gt;Start rollout&lt;/strong&gt; as shown in Figure 25. A new instance of the &lt;code&gt;rest-fights&lt;/code&gt; workload will be rolled out with the updated configuration. The rollout should take a minute or less.&lt;/li&gt; &lt;/ol&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rroll.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/rroll.png?itok=DRCyonD-" width="384" height="584" alt="The topology for rest-fights allows you to start the rollout." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 25: Start a rollout of the rest-fights service.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Re-examine the application&lt;/h2&gt; &lt;p&gt;The application is ready again when the &lt;code&gt;event-statistics&lt;/code&gt; and &lt;code&gt;rest-fights&lt;/code&gt; services have solid blue circles around their icons. Once these services are ready, go back to the main Superheroes and event statistics UIs and refresh the browser. The application should function just as before, except now the messages are being sent to a managed Kafka instance, and the schema is stored in a managed Service Registry instance.&lt;/p&gt; &lt;p&gt;Go to the &lt;a href="https://console.redhat.com/application-services/service-registry"&gt;&lt;strong&gt;Service Registry Instances&lt;/strong&gt; dashboard&lt;/a&gt; and click the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance. You should see the same &lt;code&gt;Fight&lt;/code&gt; schema as you did before with the self-deployed Apicurio instance.&lt;/p&gt; &lt;p&gt;Next, go to the &lt;a href="https://console.redhat.com/application-services/streams/kafkas"&gt;Kafka Instances dashboard&lt;/a&gt; and click the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance. You should see a bunch of metrics around the instance, topics, client connections, message rates, and more.&lt;/p&gt; &lt;h2&gt;What's next?&lt;/h2&gt; &lt;p&gt;Red Hat OpenShift Application Services delivers a streamlined developer experience for building, deploying, and scaling cloud-native applications.&lt;/p&gt; &lt;p&gt;Don't miss your chance to preview some of our new application services. Want to bring some artificial intelligence and machine learning into your applications? Want to put an API management layer in front of all your services? If so, take a look at some of the other managed &lt;a href="https://console.redhat.com/application-services/overview"&gt;OpenShift Application Services&lt;/a&gt;, such as &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-data-science"&gt;Red Hat OpenShift Data Science&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/rhoam/getting-started"&gt;Red Hat OpenShift API Management&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/28/quarkus-superheroes-managed-services-save-day" title="Quarkus Superheroes: Managed services save the day"&gt;Quarkus Superheroes: Managed services save the day&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Eric Deandrea</dc:creator><dc:date>2022-07-28T16:30:00Z</dc:date></entry><entry><title type="html">Efesto refactoring &amp;#8211; Introduction</title><link rel="alternate" href="https://blog.kie.org/2022/07/efesto-refactoring-introduction.html" /><author><name>Gabriele Cardosi</name></author><id>https://blog.kie.org/2022/07/efesto-refactoring-introduction.html</id><updated>2022-07-28T07:10:46Z</updated><content type="html">This post is meant as an introduction of the overall motivations, goals and choices around the Efesto initiative. PREMISE Originally, "Drools" (and its repository) was meant only as a "rule engine", and all the code was built around this paradigm. Over the years, new engines have been created that used, more or less, the "rule engine", or even not at all. That changed completely the actual paradigm, but the code did not reflected such a change. Different solutions or workarounds have been put in place to make this two incompatible realities (a code meant to invoke mainly the "rule engine", on one side, and different engines interacting in a coordinate manner, on the other side) works together. One of this attempt was the introduction of the "KieAssembler" (and derived) APIs. The goal was to provide a way to coordinate the execution of the different engines, but unfortunately the implementation had two flaws: * its execution has been inserted inside the code flow that was originally written for the rule engine; * it has not been adapted by all the engines, but only by the ones developed after its introduction. The result of the above is that what was meant as "coordinator" of all the different engines, became a specific sub-path of execution of the rules one; and the path of execution of the rules became, as a matter of fact, the coordinator of all the other engines. Beside that, for reasons specific to the "rule engine", the separation of a "compilation" phase and an "execution" one has never been strongly enforced, and this lack of separation leaked in the codebase. The post provides details of an analogue work done inside the Rule engine itself, showing the complexity of the task to be faced. These issues made the code hard to maintain and to expand, requiring a lot of ad-hoc solutions for problems that, actually, are inherent to the whole system. The "KieAssembler" clearly shows that. The engines that extend it have to implement the methods needed for both the compilation and the execution phase; and those KieAssembler-extending classes are invoked both at compile-time and at runtime-phase. As an example, the currently available version of tries to enforce a kind of separation with two different utility classes: * * Again, this is a downstream workaround for a design flaw. As such, each engine should write similar workarounds, and that would not solve the root cause. Consequence of that is that different engines follows different designs and address the same needs in different ways. Some attempts have been made to address these shortcomings, but at a downstream level, and for a more or less specific use-case, making those attempts less efficient then expected. The best example of this is the project. The goal of the "Efesto" refactoring is to tackle all the mentioned issues at the root, adapting the overall codebase to the current paradigm by which the different components are used, following the hard lessons learned over the years. DICTIONARY We define a domain dictionary here because, over the years, some terms have been used with different meanings in different situations, and almost always misunderstanding arose due to these different interpretations. The following are the definition and meanings used in this series of posts: * Model: the textual representation of a given model; e.g. Rules (DRL, other), Decision (DMN), Predictions (PMML), Workflow (BPMN, other) * Engine: the code needed to * transform a specific model in executable form; * execute the executable form of a specific model with a given input data * some examples: * Rule engine * Decision engine * Prediction engine * Workflow engine * Efesto: the framework that exposes the functionalities of the different engines and the name of the project that contains the refactoring * Compile-time: the process of transform the original model in executable form * Runtime: the process of executing a given model with a user input * Container: a given application that uses the drools functionalities (compilation and/or runtime) to fulfill its scope; some examples: * Kie-maven-plugin: uses compile-time to retrieve bytecode and then dump it to a kjar; * Kie-server: uses runtime to load/execute kjars (and, eventually, compile-time for on-the-fly compilation/reload) * Kogito-build: uses compile-time to retrieve bytecode and then dump it to a jar/native image; * Kogito-execution: uses runtime to load/execute jar/native image CLEAN ARCHITECTURE PRINCIPLES The main goal is to have a modular, decoupled system that will be easy to maintain in the long term (i.e. fixing bugs, improving performance, adding features). To achieve that, the knowledge relationship between the different parts is clearly defined and enforced. The system has core components and peripheral components. The “knowledge” arrow points only inward, i.e. peripheral components have knowledge of core components, but not the other way around. Peripheral components does not have knowledge of each other. MICROKERNEL STYLE The microkernel/plugin design is used to reflect the relationship between the different engines and the overall system. Every engine is implemented as a plugin component, and no direct relationship exists between plugins. MAIN TASKS The goal of Efesto refactoring are: * Separate what is “Drools” and what is not Drools * Separate compilation/execution phases * Enforce engines consistency * Provide a pluggable/chainable design SEPARATE DROOLS/NOT DROOLS Efesto (the framework, as defined in this post) is considered an agnostic provider of model execution. As such, it does not depend on any other framework, and it is available as a standalone library, runnable inside any kind of environment/container (e.g. Spring, Quarkus, Kogito, KieServer, etc). To allow that, it contains the bare-minum code required to coordinate the transformation of models in unit of executions, and the execution of them to provide a result. One consequence of this approach is that some functionalities, that are currently in charge of the drools code, will be delegated to the "container". As example, the framework does not write compiled classes to the filesystem, but delegates this task to the invoking code, like the KieMaven plugin. The reason behind this specific choice is that write to a filesystem, and relying on that, requires a series of assumptions (firt of all, a read-write environment) that are not absolutely granted, and should not be addressed by the framework itself, but by the container it is used in. SEPARATE COMPILATION/EXECUTION PHASES As defined before, compilation is the process of transforming a model to an executable unit. Usually it involves some code-generation, but this is not mandatory at all. The result of a compilation is stored inside a so-called "IndexFile", that is a registry of the generated resources, and also contains the entry-point for the execution. As a matter of fact, this entry-point could be a code-generated class, but also an already-existing one (e.g. DMN). On the other side, execution is the process of receiving input data, submitting it to unit of execution, and returning a result. In this phase, the framework reads the identifier of the resource to be invoked from the input; then, the required engine reads the informations needed for the invocation of the entry point from the IndexFile. ENFORCE ENGINES CONSISTENCY Every engine follows the same design. This means that inside the Drools framework there is not a preferential path of execution, tailored around one specific engine, to which all the others have to adapt. Instead, they all implements the same common API, so that the flow of execution is the same for every one. At the same time, this requires and enforces independency between the engines. Every engine implements a “compilation” service and a “loading” service: the former responsible of compiled-resource generation (e.g. code-generation, class compilation, entry-point definition); the latter responsible for actual entry-point invocation. PROVIDE A PLUGGABLE/CHAINABLE DESIGN The microkernel architecture allows the implementation of different engines as isolated plugins. That, in turns, provides some out-of-the-box features: * parallel development of different engines, avoiding overlapping/conflict issues * incremental implementation of new engines, without the needs of a BigBang release * no Monolithic design, where every component is bound, directly on indirectly, to the others * different implementation for the same engine, delegating the choice to the container (with the maven dependency mechanism) * allows “customer” to implement their own version/customization for a given engine The "chainable" feature refers to the possibility to invoke one engine from another. This is a well-known requirement at execution time (e.g. DMN engine requires PMML engine evaluation), but also at compile-time. Since part of execution could be delegated to another engine, this implies that the invoked engine should have "compiled" that part of execution (whatever this mean in specific cases). Another interesting use case is to compile different resources to the same engine. An example of this is offered by Rule engine. THE RULE ENGINE USE-CASE The Rule engine actually has different "formats": Drl files, Decision tables, etc.. All this models are "translated" to a PackageDescr at a given point; and the final result is always the same, an Executable model. For each kind of source there is a specific implementation responsible to translate it to a PackageDescr. There is also an implementation that takes as input the PackageDescr and returns the Executable model. So, the different model-specific engines translates the input to a PackageDescr, and then delegates to the latter one to transform it to the final Executable model. As a by-side note, that chainability feature provides an extremely easy and fast way to manage any kind of "definition" as "Rules" (or whatever engine). CONCLUSION This is the first post of a series around the Efesto effort and implementation. Following ones will go deeper inside technical details and will provide some real use-cases and code so… stay tuned!!! The post appeared first on .</content><dc:creator>Gabriele Cardosi</dc:creator></entry><entry><title type="html">How to run Java Mission Control in Eclipse</title><link rel="alternate" href="http://www.mastertheboss.com/java/how-to-run-java-mission-control-in-eclipse/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/how-to-run-java-mission-control-in-eclipse/</id><updated>2022-07-27T15:02:27Z</updated><content type="html">This article continues our learning through the Java Mission Control (JMC) tool. Within it, we will learn how to run JMC as standalone application or as Eclipse IDE plugin. Firstly, if you are new to Java Mission Control, we recommend checking this article for a brief introduction to it: How to use Java Mission Control ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>SaaS security in Kubernetes environments: A layered approach</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/27/saas-security-kubernetes-environments-layered-approach" /><author><name>Alex Kubacki</name></author><id>265fb1fc-a3c2-44d1-a2d0-41d140c6ac79</id><updated>2022-07-27T07:00:00Z</updated><published>2022-07-27T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/security/"&gt;Security&lt;/a&gt; is especially critical for &lt;a href="https://www.redhat.com/en/topics/cloud-computing/what-is-saas"&gt;Software-as-a-Service (SaaS)&lt;/a&gt; environments, where the platform is used by many different people who need the confidence that their data is stored safely and kept private from unrelated users. This article focuses on security concerns for &lt;a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="4093cbb4-5a74-4678-9a7a-7e3d9c8b81c1" href="https://developers.redhat.com/topics/containers" title="Building containerized applications"&gt;containers&lt;/a&gt; on your SaaS deployment running in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; environments such as &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. The article is the fifth in a series called the &lt;a href="https://developers.redhat.com/articles/2022/05/18/saas-architecture-checklist-kubernetes"&gt;SaaS architecture checklist&lt;/a&gt; that covers the software and deployment considerations for SaaS applications.&lt;/p&gt; &lt;h2&gt;Security controls and practices for SaaS&lt;/h2&gt; &lt;p&gt;Within modern enterprise environments, security needs to be built into the full life cycle of planning, development, operations, and maintenance. Good security controls and practices are critical to meeting compliance and regulatory requirements and making sure that transactions are reliable and high-performing. Security in SaaS can be broken down into five main layers: hardware, operating system, containers, Kubernetes, and networking. Figure 1 shows these layers and the security controls that address threats at each layer.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/layers.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/layers.png?itok=5BHUmoFN" width="478" height="829" alt="SaaS layers and their security features in Kubernetes and OpenShift." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: SaaS layers and their security features in Kubernetes and OpenShift.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Security needs to be addressed at every layer because any vulnerability in one layer could be exploited to compromise other layers. For each layer, Kubernetes and OpenShift have security controls and features that will be covered in this article. Future articles will go into more detail on specific SaaS security topics. If there are any SaaS topics for which you would like to see an article, let us know in the comments.&lt;/p&gt; &lt;h2&gt;Security at the hardware layer&lt;/h2&gt; &lt;p&gt;Securing a SaaS environment often starts with identifying where the application is going to run and the security concerns for that environment. A secure environment includes the actual data center as well as the hardware itself, including disk encryption, secure boot, BIOS-level passwords, and the use of hardware security modules (HSMs). Secrets and identity management are discussed later in this article.&lt;/p&gt; &lt;p&gt;While a lot of attention is paid to using encryption to protect &lt;em&gt;data in transit&lt;/em&gt; as it goes over the network, it is also critical to protect &lt;em&gt;data at rest&lt;/em&gt; as it is stored on physical storage devices in data centers. The risks to data at rest are much higher in data centers where you lack control over access to the facility and where third-party contractors may be employed. Use disk encryption to secure data at rest by protecting the data stored on the physical server from unintended access.&lt;/p&gt; &lt;p&gt;An HSM is typically a physical device that securely stores digital keys through encryption to protect sensitive data. HSMs are used to manage and safeguard security credentials, keys, certificates, and secrets while at rest and in transit. The HSM provides an increased level of protection over software-only approaches such as a secrets vault.&lt;/p&gt; &lt;p&gt;Cloud HSMs are available from the major cloud providers to provide increased protection in cloud environments. HSMs are recommended to manage secrets in SaaS environments.&lt;/p&gt; &lt;p&gt;Protect access to the server by enabling secure boot and using BIOS-level passwords. Secure boot is a firmware security feature of the Unified Extensible Firmware Interface (UEFI) that makes sure that only immutable and signed software can be run during boot.&lt;/p&gt; &lt;p&gt;For more information, check out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/identity-and-access-devsecops-life-cycle"&gt;Identity and access in the DevSecOps life cycle&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://cloud.redhat.com/blog/self-contained-ready-and-secured-enhancing-red-hat-openshift-with-hardware-cryptography"&gt;Enhancing Red Hat OpenShift with Hardware Cryptography&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/security_hardening/assembly_securing-rhel-during-installation-security-hardening#BIOS_and_UEFI_security_securing-rhel-during-installation"&gt;Securing Red Hat Enterprise Linux during installation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Operating system security&lt;/h2&gt; &lt;p&gt;Every Kubernetes cluster runs on top of some underlying operating system (OS). Security features and hardening at the OS layer help protect the overall cluster, so it is important to enable and use OS-level controls.&lt;/p&gt; &lt;p&gt;When it comes to security hardening at the OS level, Red Hat OpenShift has two distinct advantages. First, &lt;a href="https://www.redhat.com/en/topics/linux/what-is-selinux"&gt;Security-Enhanced Linux&lt;/a&gt; (SELinux) is integrated and enabled out of the box. Second, OpenShift runs on Red Hat Enterprise Linux CoreOS, a unique OS image tuned for SaaS use.&lt;/p&gt; &lt;h3&gt;Security-enhanced Linux&lt;/h3&gt; &lt;p&gt;SELinux is a security architecture for &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; systems that grants administrators finer-grained control over access to system resources than is available with default Linux. SELinux defines mandatory access controls for applications, processes, and files on a system. On a Kubernetes node, SELinux adds an important layer of protection against &lt;a href="https://www.redhat.com/en/blog/latest-container-exploit-runc-can-be-blocked-selinux"&gt;container-breakout vulnerabilities&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Thus, one of the most effective security measures is to enable and configure SELinux, which Red Hat has made standard on all OpenShift clusters. It is considered a best practice to use SELinux in SaaS environments. In OpenShift, SELinux enhances container security by ensuring true container separation and mandatory access control.&lt;/p&gt; &lt;p&gt;For more information, see:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/how-selinux-separates-containers-using-multi-level-security"&gt;How SELinux separates containers using Multi-Level Security&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/why-you-should-be-using-multi-category-security-your-linux-containers"&gt;Why you should be using Multi-Category Security for your Linux containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/using-container-technology-make-trusted-pipeline"&gt;Using container technology to make a more secure pipeline&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/network-traffic-control-containers-red-hat-openshift"&gt;Network traffic control for containers in Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;A hardened OS for containers: Red Hat Enterprise Linux CoreOS&lt;/h3&gt; &lt;p&gt;OpenShift's operating system, Red Hat Enterprise Linux CoreOS, is based on Red Hat Enterprise Linux and uses the same kernel, code, and open source development processes. This special version ships with a specific subset of Red Hat Enterprise Linux packages, designed for use in OpenShift 4 clusters. The key features that make this operating system more secure are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Based on Red Hat Enterprise Linux: The underlying OS is primarily Red Hat Enterprise Linux components, which means it has the same quality, security, control measures, and support. When a fix is pushed to Red Hat Enterprise Linux, that same fix is pushed to Red Hat Enterprise Linux CoreOS.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Controlled immutability: Red Hat Enterprise Linux CoreOS is managed via OpenShift APIs, which leads to more hands-off operating system management. Management is primarily performed in bulk for all nodes throughout the OpenShift cluster. The latest state of the Red Hat Enterprise Linux CoreOS system is stored on the cluster, making it easy to add new nodes or push updates to all nodes. Given the OS's centralized management and transactional nature, only a few system settings can be modified on a Red Hat Enterprise Linux CoreOS installation.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Command-line container tools: Red Hat Enterprise Linux CoreOS includes container tools compatible with the &lt;a href="https://opencontainers.org"&gt;Open Container Initiative&lt;/a&gt; (OCI) specification to build, copy, and manage container images. Many container runtime administration features are available through Podman. The &lt;a href="https://www.redhat.com/sysadmin/how-run-skopeo-container"&gt;skopeo&lt;/a&gt; command copies, authenticates, and signs images. The &lt;a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/"&gt;crictl&lt;/a&gt; command lets you view and troubleshoot containers and pods.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Robust transactional updates: Red Hat Enterprise Linux CoreOS offer the &lt;a href="https://coreos.github.io/rpm-ostree/"&gt;rpm-ostree&lt;/a&gt; upgrade process, which assures that an upgrade takes place atomically. If something goes wrong, the original OS can be restored in a single rollback.&lt;/p&gt; &lt;p&gt;OpenShift handles OS upgrades through the &lt;a href="https://github.com/openshift/machine-config-operator"&gt;Machine Config Operator&lt;/a&gt; (MCO), which encompasses a complete OS upgrade instead of individual packages as in traditional Yum upgrades. OpenShift also updates nodes via a rolling update to mitigate the updates' impact and maintain cluster capacity. During installation and upgrades, the latest immutable filesystem tree is read from a container image, written to disk, and loaded to the bootloader. The machine will reboot into the new OS version, guaranteeing an atomic update.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Security during cluster installation: Red Hat Enterprise Linux CoreOS minimizes security decisions during installation. Two security features are considered pre-first boot decisions for cluster operations: &lt;a href="https://docs.openshift.com/container-platform/4.8/installing/installing-fips.html#installing-fips"&gt;support for FIPS cryptography&lt;/a&gt; and full disk encryption (FDE). After the cluster is bootstrapped, the cluster can further be configured for other node-level changes.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Container layer&lt;/h2&gt; &lt;p&gt;The container layer in Kubernetes and OpenShift isolates processes from one another and from the underlying OS. Instead of traditional software design, where all the components are linked, deployed together, and ultimately dependent on each other, containers are independent, resulting in smaller impacts. If one container goes down, it can easily be replaced. If a container image is found to have a security flaw, the flaw is isolated to that image and requires updating only that image rather than the whole cluster.&lt;/p&gt; &lt;p&gt;Red Hat OpenShift has many features that improve container security for multitenant environments.&lt;/p&gt; &lt;h3&gt;Container engine&lt;/h3&gt; &lt;p&gt;A &lt;em&gt;container engine&lt;/em&gt; provides tools for creating container images and starting containers. In OpenShift, the default container engine is &lt;a href="https://docs.openshift.com/container-platform/3.11/crio/crio_runtime.html"&gt;CRI-O&lt;/a&gt;, which supports containers conforming to OCI and libcontainerd. The container engine focuses on the features needed by Kubernetes's &lt;a href="https://kubernetes.io/docs/concepts/architecture/cri/"&gt;Container Runtime Interface&lt;/a&gt; (CRI). This customized container engine shrinks the surface available to a security attack, because the container engine does not contain unneeded features such as direct command-line use or orchestration facilities.&lt;/p&gt; &lt;p&gt;We have also aligned the CRI more with Kubernetes: Updates to CRI-O are made to work better with the current Kubernetes release.&lt;/p&gt; &lt;h3&gt;Container security in the Linux kernel&lt;/h3&gt; &lt;p&gt;The kernel offers features to ensure the security of containers and everything else running on the OS. First off, all containers are launched inside a namespace that creates an isolated sandbox segregating the containers, files systems, processes, and networking.&lt;/p&gt; &lt;p&gt;The next feature is &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch01"&gt;control groups&lt;/a&gt; (cgroups), which isolate hardware resource sharing between containers and nodes of the OpenShift cluster. The use of cgroups prevents any single process or container from using up all the available resources on a host.&lt;/p&gt; &lt;p&gt;Finally, as we discussed earlier, Red Hat Enterprise Linux CoreOS enables SELinux, which prevents a container from breaking its isolation and thus interfering indirectly with other containers on the same host.&lt;/p&gt; &lt;h2&gt;Cluster security on Kubernetes and Red Hat OpenShift&lt;/h2&gt; &lt;p&gt;The cluster level controls how Kubernetes deploys hosts, manages shared resources, controls intercontainer communications, manages scaling, and controls access to the cluster. An OpenShift cluster is made up of a control plane, worker nodes, and any additional resources needed. The following subsections cover some of the security concerns for the different aspects of the cluster.&lt;/p&gt; &lt;h3&gt;Control plane isolation&lt;/h3&gt; &lt;p&gt;It is considered a best practice to isolate the cluster's control plane nodes from the worker nodes. This is usually done using separate hardware for the control plane to mitigate the impact of any misconfiguration, resource management problems, or vulnerabilities.&lt;/p&gt; &lt;h3&gt;Identity management&lt;/h3&gt; &lt;p&gt;Every Kubernetes cluster needs some form of identity management. Out of the box, Red Hat OpenShift comes with a default &lt;a href="https://oauth.net"&gt;OAuth&lt;/a&gt; provider, which is used for token-based authentication. This provider has a single &lt;code&gt;kubeadmin&lt;/code&gt; user account, which you can use to &lt;a href="https://docs.openshift.com/container-platform/4.10/authentication/understanding-identity-provider.html"&gt;configure an identity provider via a custom resource (CR)&lt;/a&gt;. OpenShift supports &lt;a href="https://openid.net/connect/"&gt;OpenID Connect&lt;/a&gt; and LDAP standard identity providers. After identities are defined, use role-based access control (RBAC) to define and apply permissions.&lt;/p&gt; &lt;h3&gt;Cluster access control&lt;/h3&gt; &lt;p&gt;Before users interact with the cluster, they first must authenticate via the OAuth server. Internal connections to the API server are authenticated using X.509 certificates.&lt;/p&gt; &lt;h3&gt;Security context constraints&lt;/h3&gt; &lt;p&gt;&lt;a href="https://docs.openshift.com/container-platform/4.8/authentication/managing-security-context-constraints.html"&gt;Security context constraints&lt;/a&gt; (SCCs) are an OpenShift security feature that limits a pod's resource access and allowable actions. SCCs let administrators control much of the pod's configuration, such as the SELinux context of a container, whether a pod can run privileged containers, and the use of host directories as volumes. In OpenShift, SCCs are enabled by default and cannot be disabled. SCCs can improve isolation in SaaS deployments and reduce the impact of potential vulnerabilities.&lt;/p&gt; &lt;p&gt;Pod SCCs are determined by the group that the user belongs to as well as the service account, if specified. By default, worker nodes and the pods running on them receive an SCC type of &lt;code&gt;restricted&lt;/code&gt;. This SCC type prevents pods from running as privileged and requires them to run under a UID that is selected at runtime from a preallocated range of UIDs.&lt;/p&gt; &lt;h3&gt;Secrets&lt;/h3&gt; &lt;p&gt;In SaaS deployments, the tenants need to secure their sensitive data on the cluster. This is handled with Secret objects on OpenShift. Secret objects hold sensitive information such as passwords, OCP client configuration files, private source repository credentials, etc. This way of using Secret objects decouples the sensitive content from the pods.&lt;/p&gt; &lt;p&gt;When the sensitive content is needed, it can be mounted to the container via a volume plugin, or the system can use the secrets to perform the action on behalf of the pod. Key properties of secrets include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Secret data can be created by one entity, such as a configuration tool, and referred to by another, such as an application.&lt;/li&gt; &lt;li&gt;Secret data volumes are backed by temporary file-storage facilities (tmpfs) and never come to rest on a node.&lt;/li&gt; &lt;li&gt;Secret data can be shared within a namespace.&lt;/li&gt; &lt;li&gt;Secret data can &lt;a href="https://docs.openshift.com/container-platform/4.10/security/encrypting-etcd.html"&gt;optionally be encrypted at rest&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more information, read &lt;a href="https://docs.openshift.com/container-platform/4.10/nodes/pods/nodes-pods-secrets.html"&gt;Providing sensitive data to pods&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Red Hat Advanced Cluster Security for Kubernetes&lt;/h3&gt; &lt;p&gt;In addition to the standard security features in Red Hat OpenShift, Red Hat offers additional products to enhance the security of the platform. One of those is &lt;a href="https://cloud.redhat.com/products/kubernetes-security"&gt;Red Hat Advanced Cluster Security for Kubernetes&lt;/a&gt; (previously StackRox). Red Hat Advanced Cluster Security for Kubernetes protects your vital applications across the build, deploy, and runtime stages. It deploys in your infrastructure and easily integrates with &lt;a href="https://developers.redhat.com/topics/devops/"&gt;DevOps&lt;/a&gt; tooling and workflows. This integration makes it easy to apply security and compliance policies.&lt;/p&gt; &lt;p&gt;Red Hat Advanced Cluster Security adds to OpenShift's built-in security by improving the following core tenants of security:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Improving &lt;em&gt;visibility&lt;/em&gt; of the environment, so administrators can more easily detect issues as they happen.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Managing vulnerabilities&lt;/em&gt; once they have been identified by deploying fixes via an integrated &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt; pipeline.&lt;/li&gt; &lt;li&gt;Ensuring &lt;em&gt;compliance&lt;/em&gt; with industry standards and best practices.&lt;/li&gt; &lt;li&gt;Adding robust &lt;em&gt;network segmentation&lt;/em&gt; to restrict network traffic to only the necessary uses.&lt;/li&gt; &lt;li&gt;A &lt;em&gt;risk-based &lt;/em&gt;ranking of each deployment to determine the likelihood of a security risk, helping to ensure that the highest risk deployments get immediate remediation first.&lt;/li&gt; &lt;li&gt;Identifying misconfigurations and evaluating role-based access control (RBAC) access for users via &lt;em&gt;configuration management, &lt;/em&gt;to ensure that the configuration meets best practices.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Runtime detection and response&lt;/em&gt; to automatically identify abnormal actions that could indicate a security breach or misuse of the environment.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To learn more, see &lt;a href="https://cloud.redhat.com/blog/a-brief-introduction-to-red-hat-advanced-cluster-security-for-kubernetes"&gt;A Brief Introduction to Red Hat Advanced Cluster Security for Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Networking layer&lt;/h2&gt; &lt;p&gt;The networking layer is the outermost layer of a security architecture. The network is where most IT security attacks occur, due to misconfiguration and vulnerabilities. Proper planning and configuration of the network security layer components ensure that the environment is secure. Kubernetes has software-defined networking (SDN) controls that can improve network security in SaaS deployments. Red Hat OpenShift provides additional controls that build on what's available in Kubernetes.&lt;/p&gt; &lt;h3&gt;Network policy&lt;/h3&gt; &lt;p&gt;A network policy controls the traffic between pods by defining the permissions they need in order to communicate with other pods and network endpoints. OpenShift expands on policies by logically grouping components and rules into collections for easy management.&lt;/p&gt; &lt;p&gt;It is worth noting that network policies are additive. Therefore, when you create multiple policies on one or more pods, the union of all rules is applied regardless of the order in which you list them. The resulting pod behavior reflects every allow and deny rule for ingress and egress.&lt;/p&gt; &lt;h3&gt;Container network interface&lt;/h3&gt; &lt;p&gt;In a Kubernetes cluster, by default, pods are attached to a single network and have a single container network interface (CNI). The CNI manages the network connectivity of containers and removes resources when containers are deleted.&lt;/p&gt; &lt;p&gt;Kubernetes uses SDN plugins to implement the CNI. They manage the resources of the network interfaces for new pods. The CNI plugins set up proper networking constructs for pod-to-pod and pod-to-external communication and enforce network policies.&lt;/p&gt; &lt;h3&gt;Openshift networking security features&lt;/h3&gt; &lt;p&gt;OpenShift offers the following additional features and components to secure networks for cloud-native deployments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Network operations: OpenShift includes a set of operators that manage networking components to enforce best practices and mitigate human errors.&lt;/li&gt; &lt;li&gt;Multiple network interfaces: The Kubernetes default is for all pods to use a single network and a single primary network interface, but with OpenShift, you can configure additional network interfaces. This allows network optimization to improve performance and enhances isolation to improve security.&lt;/li&gt; &lt;li&gt;Ingress security enhancements: OpenShift exposes the cluster to external resources or clients via a &lt;em&gt;route&lt;/em&gt; resource. Routes provide advanced features not found in a standard Kubernetes Ingress controller, including TLS re-encryption, TLS passthrough, and split traffic for blue-green deployments.&lt;/li&gt; &lt;li&gt;Egress security enhancements: While the default OpenShift rule allows all egress traffic to leave the cluster with no restrictions, OpenShift has tools for fine-grained control and filtering of outbound traffic. OpenShift lets you control egress traffic via an &lt;a href="https://docs.openshift.com/container-platform/4.6/networking/openshift_sdn/configuring-egress-firewall.html"&gt;egress firewall&lt;/a&gt;, &lt;a href="https://docs.openshift.com/container-platform/4.6/networking/openshift_sdn/using-an-egress-router.html"&gt;egress routers&lt;/a&gt;, and &lt;a href="https://docs.openshift.com/container-platform/4.7/networking/openshift_sdn/assigning-egress-ips.html"&gt;egress static IP addresses&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Service mesh: Red Hat OpenShift Service Mesh, based on the &lt;a href="https://istio.io"&gt;Istio&lt;/a&gt; project, adds a transparent layer to existing application network services running in a cluster, allowing complex management and monitoring without requiring changes to the services. The service mesh does this by deploying a sidecar proxy alongside the relevant services to intercept and manage all network communications. With Red Hat OpenShift Service Mesh, you can create a network with the following services: discovery, load balancing, service-to-service authentication, failure recovery, metrics, monitoring, A/B testing, canary releases, rate limiting, access control, and end-to-end authentication.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more information, see the &lt;a href="https://www.redhat.com/rhdc/managed-files/cl-openshift-security-guide-ebook-us287757-202103.pdf"&gt;Red Hat OpenShift Security Guide&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Partner with Red Hat to build your SaaS&lt;/h2&gt; &lt;p&gt;This article covered controls that can be used to improve the security of your SaaS deployment at the hardware, OS, container, Kubernetes cluster, and network levels. Future articles will go deeper into SaaS security topics.&lt;/p&gt; &lt;p&gt;&lt;a href="https://connect.redhat.com/en/partner-with-us/red-hat-saas-foundations"&gt;Red Hat SaaS Foundations&lt;/a&gt; is a partner program designed for building enterprise-grade SaaS platforms on Red Hat OpenShift or Red Hat Enterprise Linux, and deploying them across multiple cloud and non-cloud footprints. &lt;a href="http://https//mail.google.com/mail/?view=cm&amp;fs=1&amp;tf=1&amp;to=saas@redhat.com"&gt;Email&lt;/a&gt; us to learn more.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/27/saas-security-kubernetes-environments-layered-approach" title="SaaS security in Kubernetes environments: A layered approach"&gt;SaaS security in Kubernetes environments: A layered approach&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Alex Kubacki</dc:creator><dc:date>2022-07-27T07:00:00Z</dc:date></entry><entry><title type="html">Vert.x virtual threads incubator</title><link rel="alternate" href="https://vertx.io/blog/vertx-virtual-threads-incubator" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/vertx-virtual-threads-incubator</id><updated>2022-07-27T00:00:00Z</updated><content type="html">JEP 425: Virtual threads aka Loom is coming to the Java Platform as a preview in Java 19. The Vert.x team is launching an incubator project to experiment with virtual threads.</content><dc:creator>Julien Viet</dc:creator></entry><entry><title type="html">How to use Java Mission Control to monitor Java apps</title><link rel="alternate" href="http://www.mastertheboss.com/java/how-to-use-java-mission-control-to-monitor-java-apps/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/how-to-use-java-mission-control-to-monitor-java-apps/</id><updated>2022-07-25T07:37:43Z</updated><content type="html">This article is whirlwind tour across the Java Flight Recorder (JFR) and the Java Mission Control (JMC) suite. At the end of it, you will be able to monitor, collect diagnostic data and profile any running Java application. What is Java Flight Recorder? Firstly, why do we need another tool to monitor Java ? As ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">RESTEasy Releases</title><link rel="alternate" href="https://resteasy.github.io/2022/07/21/resteasy-releases/" /><author><name /></author><id>https://resteasy.github.io/2022/07/21/resteasy-releases/</id><updated>2022-07-21T18:11:11Z</updated><dc:creator /></entry><entry><title>How to use Go Toolset container images</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/21/how-use-go-toolset-container-images" /><author><name>Alejandro Sáez Morollón</name></author><id>011e5016-34ab-4570-8e1a-575cc4281eec</id><updated>2022-07-21T07:00:00Z</updated><published>2022-07-21T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_developer_tools/2019.1/html-single/using_go_toolset/index"&gt;Go Toolset package&lt;/a&gt; delivers the &lt;a href="https://developers.redhat.com/topics/go"&gt;Go language&lt;/a&gt; with Federal Information Processing Standard (FIPS) support for cryptographic modules and the &lt;a href="https://github.com/go-delve/delve"&gt;Delve debugger&lt;/a&gt; to &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; customers. We introduced this package &lt;a href="https://developers.redhat.com/blog/2017/10/31/getting-started-go-toolset"&gt;a few years ago&lt;/a&gt;. Now we also provide Go Toolset in container images. This article illustrates how these images support modern Go development.&lt;/p&gt; &lt;h2&gt;Obtaining Go Toolset container images&lt;/h2&gt; &lt;p&gt;The images are in Red Hat's &lt;a href="https://catalog.redhat.com/software/containers/explore"&gt;container image catalog&lt;/a&gt;. Search for &lt;code&gt;go-toolset&lt;/code&gt; &lt;a href="https://catalog.redhat.com/software/containers/search?q=go-toolset"&gt;here&lt;/a&gt;. As of this writing, we have images based on Red Hat Enterprise Linux 7, Red Hat Enterprise Linux 8, and &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Images&lt;/a&gt; (UBI) 7 and 8. To learn more about UBI, check the article &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat UBI is a Verified Publisher on Docker Hub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you don't want to use the Go Toolset container image, you can still install the Go Toolset package inside a ubi8 container. Just run &lt;code&gt;dnf install go-toolset&lt;/code&gt; inside the &lt;code&gt;registry.access.redhat.com/ubi8&lt;/code&gt; image, and you'll be all set.&lt;/p&gt; &lt;h2&gt;Pull the latest version of the Go Toolset image&lt;/h2&gt; &lt;p&gt;Using your container engine is the best way to pull down a container image. Let's use &lt;a href="https://podman.io"&gt;Podman&lt;/a&gt; for this example. The following commands pull down the latest version of the image based on UBI 8 and run a shell inside it:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;[alex@lab ~]$ podman pull registry.access.redhat.com/ubi8/go-toolset:latest [alex@lab ~]$ podman run --rm -it go-toolset /bin/bash bash-4.4$ go version go version go1.17.7 linux/amd64 bash-4.4$ exit exit [alex@lab ~]$ &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Using the image in a multistage environment&lt;/h2&gt; &lt;p&gt;You can refer to the image in a Dockerfile like any other image and use it, for example, as a build step in a multistage Dockerfile in tandem with the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/ubi-micro/5ff3f50a831939b08d1b832a"&gt;Red Hat Universal Base Image 8 Micro image&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="Dockerfile"&gt;FROM ubi8/go-toolset as build COPY ./src . RUN go mod init my_app &amp;&amp; \ go mod tidy &amp;&amp; \ go build . FROM ubi8/ubi-micro COPY --from=build /opt/app-root/src/my_app . CMD ./my_app &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The ubi-micro image takes up less than 40MB, so the surface of your container is tiny but holds all of the great features UBI delivers.&lt;/p&gt; &lt;h2&gt;Go Toolset works with Toolbox&lt;/h2&gt; &lt;p&gt;I won’t be lying if I say &lt;a href="https://github.com/containers/toolbox"&gt;Toolbox&lt;/a&gt; is one of my favorite software tools. It allows you to keep working with your files and configurations inside a new container. I use Toolbox every day, and it works wonderfully with the Go Toolset image.&lt;/p&gt; &lt;p&gt;You can install Toolbox in both Red Hat Enterprise Linux and Fedora with &lt;code&gt;dnf install toolbox&lt;/code&gt;. The &lt;code&gt;toolbox&lt;/code&gt; command lets you install other resources:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;[alex@lab ~]$ cat /etc/redhat-release Fedora release 35 (Thirty Five) [alex@lab ~]$ toolbox create --image registry.access.redhat.com/ubi8/go-toolset [alex@lab ~]$ toolbox enter go-toolset [alex@toolbox ~]$ cat /etc/redhat-release Red Hat Enterprise Linux release 8.6 (Ootpa) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you have all the files, configurations, and packages included with the image.&lt;/p&gt; &lt;h2&gt;Attach VS Code for IDE support&lt;/h2&gt; &lt;p&gt;You can improve productivity by attaching Visual Studio Code, &lt;a href="https://developers.redhat.com/products/vscode-extensions/overview"&gt;VS Code&lt;/a&gt; to a running container. Install the &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers"&gt;Remote Containers extension&lt;/a&gt; and execute &lt;strong&gt;Attach to Running Container&lt;/strong&gt;. There is no need to configure Git or Kerberos tokens; simply jump into the container and start working.&lt;/p&gt; &lt;p&gt;I use this process to play with the new versions of the container images we built and bootstrap projects such as Go.&lt;/p&gt; &lt;h2&gt;Go Toolset includes FIPS security&lt;/h2&gt; &lt;p&gt;One exciting feature supported by the golang package in Go Toolset is &lt;a href="https://www.redhat.com/en/blog/how-rhel-8-designed-fips-140-2-requirements"&gt;FIPS 140-2 cryptographic modules&lt;/a&gt;. You can expect Go Toolset to follow the FIPS 140-2 security standard. Check for FIPS mode inside the Go Toolset container image:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;[alex@lab ~]$ fips-mode-setup --check FIPS mode is enabled. [alex@lab ~]$ toolbox enter go-toolset [alex@toolbox ~]$ fips-mode-setup --check FIPS mode is enabled. &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Enterprise-ready Go in Red Hat Enterprise Linux&lt;/h2&gt; &lt;p&gt;The Go Toolset package is available as an image and integrates smoothly with other popular developer tools. By following the instructions I have provided, you can quickly become a more productive Go programmer in the cloud.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/21/how-use-go-toolset-container-images" title="How to use Go Toolset container images"&gt;How to use Go Toolset container images&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Alejandro Sáez Morollón</dc:creator><dc:date>2022-07-21T07:00:00Z</dc:date></entry><entry><title>Get started with OpenShift Service Registry</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/10/11/get-started-openshift-service-registry" /><author><name>Evan Shortiss</name></author><id>fa17af9f-46e7-4502-8b24-a99e6890c3ed</id><updated>2022-07-20T16:00:00Z</updated><published>2022-07-20T16:00:00Z</published><summary type="html">&lt;p&gt;Red Hat OpenShift Service Registry is a fully hosted and managed service that provides an API and schema registry for &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;. OpenShift Service Registry makes it easy for development teams to publish, discover, and reuse APIs and schemas.&lt;/p&gt; &lt;p&gt;Well-defined API and schema definitions are essential to delivering robust microservice and event streaming architectures. Development teams can use a registry to manage these artifacts in various formats, including &lt;a href="https://swagger.io/specification/"&gt;OpenAPI&lt;/a&gt;, &lt;a href="https://www.asyncapi.com/"&gt;AsyncAPI&lt;/a&gt;, &lt;a href="https://avro.apache.org/"&gt;Apache Avro&lt;/a&gt;, &lt;a href="https://developers.google.com/protocol-buffers"&gt;Protocol Buffers&lt;/a&gt;, and more. Data producers and consumers can then use the artifacts to validate and serialize or deserialize data.&lt;/p&gt; &lt;p&gt;This article gets you started with OpenShift Service Registry. You’ll create a &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt;-based &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; application that uses the registry to manage schemas for data sent through topics in an &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; cluster. The tutorial should take less than 30 minutes, and involves the following steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a Red Hat Hybrid Cloud account.&lt;/li&gt; &lt;li&gt;Provision an OpenShift Service Registry instance.&lt;/li&gt; &lt;li&gt;Provision an OpenShift Streams for Apache Kafka instance.&lt;/li&gt; &lt;li&gt;Create Kafka topics.&lt;/li&gt; &lt;li&gt;Create a service account to facilitate authenticated access to your Kafka and Service Registry instances.&lt;/li&gt; &lt;li&gt;Build and run a Java application.&lt;/li&gt; &lt;/ol&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Schemas and API definitions are metadata that represent a contract between decoupled services, so they must be discoverable, documented, and assigned versions to track their evolution over time.&lt;/p&gt; &lt;h2&gt;About OpenShift Service Registry&lt;/h2&gt; &lt;p&gt;Red Hat OpenShift Service Registry is based on the open source &lt;a href="https://www.apicur.io/registry/"&gt;Apicurio Registry project&lt;/a&gt;. It provides a highly available service registry instance that is secure and compatible with both the &lt;a href="https://docs.confluent.io/platform/current/schema-registry/develop/api.html"&gt;Confluent Schema Registry API&lt;/a&gt; and &lt;a href="https://github.com/cloudevents/spec/blob/master/schemaregistry/schemaregistry.md"&gt;CNCF Schema Registry API&lt;/a&gt;. OpenShift Service Registry is also a perfect companion service for applications that use &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/red-hat-openshift-api-management/overview"&gt;Red Hat OpenShift API Management&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;You need a Red Hat Hybrid Cloud account to run the examples in this article. Create an account for free at &lt;a href="https://console.redhat.com/"&gt;console.redhat.com&lt;/a&gt;. You also need the following tools in your development environment:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Java 11 or higher&lt;/li&gt; &lt;li&gt;Maven 3.8.1 or higher&lt;/li&gt; &lt;li&gt;Git&lt;/li&gt; &lt;li&gt;Your favorite IDE or text editor&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Creating a Service Registry instance&lt;/h2&gt; &lt;p&gt;Organizations or individuals with a Red Hat Hybrid Cloud account are entitled to a two-month trial instance of OpenShift Service Registry. To create an instance:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Log in to your account on &lt;a href="https://console.redhat.com"&gt;console.redhat.com&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;In the user interface (UI), select &lt;strong&gt;Application Services&lt;/strong&gt; from the menu on the left.&lt;/li&gt; &lt;li&gt;Expand the &lt;strong&gt;Service Registry&lt;/strong&gt; entry on the side menu and click the &lt;strong&gt;Service Registry Instances&lt;/strong&gt; link. Acknowledge the warning that it is a beta service.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create Service Registry instance&lt;/strong&gt; button. A modal dialog will be displayed.&lt;/li&gt; &lt;li&gt;Enter a name for your Service Registry instance and click the &lt;strong&gt;Create&lt;/strong&gt; button.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Your OpenShift Service Registry instance will be ready to use in a minute or two. A green checkmark will be displayed in the &lt;strong&gt;Status&lt;/strong&gt; column to indicate when the instance is ready, as shown in Figure 1.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/service_registry_ready.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/service_registry_ready.png?itok=2I9j3fwW" width="1440" height="807" alt="When a green checkmark and a Ready status are displayed in the OpenShift Service Registry UI, the Service Registry instance can be opened." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A Service Registry instance listed in the OpenShift Service Registry UI. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Once your instance is ready, click on its row in the portal to view connection information, as shown in Figure 2. Take note of the &lt;strong&gt;Core Registry API&lt;/strong&gt; value because you'll need it soon.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;img alt="The service Registry instance connection information displayed in the UI." data-entity-type="file" data-entity-uuid="4e2e48ce-fc0e-417f-ab08-001c0c7068ee" src="https://developers.redhat.com/sites/default/files/inline-images/dblog-screen_0_0.png" width="1660" height="938" loading="lazy" /&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The Service Registry instance connection information displayed in the UI.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Integrating Java applications with Service Registry&lt;/h2&gt; &lt;p&gt;Kafka producer applications can use serializers to encode messages that conform to a specific event schema. Kafka consumer applications can then use deserializers to validate that messages were serialized using the correct schema, based on a specific schema ID. This process is illustrated in Figure 3. You'll test serialization and deserialization using Java producer and consumer applications that connect to Kafka.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kafka_schema.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/kafka_schema.png?itok=8Rp6rq9H" width="1440" height="841" alt="Both producers and consumers in Kafka get schemas from the OpenShift Service Registry." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Kafka producer and consumer applications using the OpenShift Service Registry to share schemas. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h3&gt;Provision a managed Kafka instance and create topics&lt;/h3&gt; &lt;p&gt;To get started, you'll need to create an OpenShift Streams for Apache Kafka instance and two topics: one named &lt;code&gt;quote-requests&lt;/code&gt; and the other named &lt;code&gt;quotes&lt;/code&gt;. We've explained how to obtain this runtime environment for free in &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka#provision_a_kafka_cluster_with_openshift_streams_for_apache_kafka_through_the_ui"&gt;this article&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Remember to take note of your Kafka instance's bootstrap server URL. You will need this URL soon.&lt;/p&gt; &lt;h3&gt;Create a service account&lt;/h3&gt; &lt;p&gt;A service account is required to connect applications to the OpenShift Service Registry and OpenShift Streams for Apache Kafka instances. The service account provides a client ID and client secret that applications use to authenticate against the cloud services.&lt;/p&gt; &lt;p&gt;To create a service account:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Visit &lt;a href="https://console.redhat.com/beta/application-services/service-accounts"&gt;console.redhat.com/beta/application-services/service-accounts&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create service&lt;/strong&gt; account button.&lt;/li&gt; &lt;li&gt;Enter a name for the service account.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;The client ID and client secret will be displayed. Copy these down someplace safe.&lt;/li&gt; &lt;li&gt;Close the modal dialog.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Deploying the producer Java application&lt;/h2&gt; &lt;p&gt;At this point you have:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An OpenShift Service Registry instance&lt;/li&gt; &lt;li&gt;An OpenShift Streams for Apache Kafka instance&lt;/li&gt; &lt;li&gt;A service account for connecting applications to the previous two instances&lt;/li&gt; &lt;li&gt;Kafka topics to hold messages published by a producer&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now, it's time to deploy a producer application that publishes messages to your Kafka topic. This application utilizes an Avro schema to encode messages in Avro format. It also publishes this schema to your OpenShift Service Registry. Consumer applications can fetch the schema from OpenShift Service Registry to deserialize and validate records they consume from your Kafka topic.&lt;/p&gt; &lt;p&gt;The source code for both the producer and the consumer is available in this &lt;a href="https://github.com/evanshortiss/rhosr-quarkus-kafka-apicurio"&gt;GitHub repository&lt;/a&gt;. Clone it into your development environment:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone $REPOSITORY_URL rhosr-getting-started&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Open the &lt;code&gt;rhosr-getting-started&lt;/code&gt; project using your preferred IDE or text editor, and open the &lt;code&gt;producer/pom.xml&lt;/code&gt; file. This file contains typical dependencies that are used to connect to Kafka and expose REST services. The &lt;code&gt;quarkus-apicurio-registry-avro&lt;/code&gt; dependency is used to generate Java classes based on Avro schema definitions. It also brings in dependencies required to work with the service registry, such as service registry-aware Kafka serializers and deserializers.&lt;/p&gt; &lt;p&gt;Next, open the &lt;code&gt;producer/src/main/avro/quote.avsc&lt;/code&gt; file. This file contains an Avro schema defined using JSON. This schema can be used to generate a &lt;code&gt;Quote.java&lt;/code&gt; class that extends and implements the necessary Avro class and interface. The &lt;code&gt;Quote&lt;/code&gt; class is used to serialize outgoing messages to the underlying Kafka topic, and by the &lt;code&gt;quotes&lt;/code&gt; channel to deserialize incoming messages. The generated class can be found in the &lt;code&gt;target/generated-sources/Quota.java&lt;/code&gt; file after compiling the application or running it in development mode.&lt;/p&gt; &lt;p&gt;Lastly, examine the &lt;code&gt;producer/src/main/resource/application.properties&lt;/code&gt; file. This file configures the application to connect to a Kafka instance, register schemas with a registry, and use Avro serialization and deserialization.&lt;/p&gt; &lt;h3&gt;Run the producer application&lt;/h3&gt; &lt;p&gt;You can run the producer application wherever you like, including on an OpenShift cluster. I'll demonstrate how you can run the producer in your local development environment.&lt;/p&gt; &lt;p&gt;First, define the following environment variables in a shell. Replace the text within &lt;strong&gt;&lt;&gt;&lt;/strong&gt; angle brackets with the values you found in previous sections:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Used to authenticate against the registry and kafka cluster export CLIENT_ID=&lt;your-client-id&gt; export CLIENT_SECRET=&lt;your-client-secret&gt; # Used to connect to and authenticate against the service registry export OAUTH_SERVER_URL=https://sso.redhat.com/auth export REGISTRY_URL=&lt;core-service-registry-url&gt; # Used to connect to and authenticate against the kafka cluster export BOOTSTRAP_SERVER=&lt;kafka-bootstrap-url&gt; export OAUTH_TOKEN_ENDPOINT_URI=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Once these values are defined, you can start the producer application in the same shell using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn quarkus:dev -f ./producer/pom.xml -Dquarkus-profile=prod&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Quarkus application is now running in and has exposed an HTTP server on &lt;a href="http://localhost:8080/"&gt;http://localhost:8080/&lt;/a&gt;. Use the following command to send a POST request that creates a quote and sends it to the &lt;code&gt;quote-requests&lt;/code&gt; Kafka topic:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -X POST http://localhost:8080/quotes/request&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should receive a response in JSON format that contains a unique quote &lt;code&gt;id&lt;/code&gt; and random &lt;code&gt;price&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;View the Quote schema in Service Registry&lt;/h3&gt; &lt;p&gt;When you start the producer application and make a request to the &lt;code&gt;/quotes/request&lt;/code&gt; endpoint, the producer gets ready to send data to your Kafka topic. Prior to sending the data, the producer checks that the Quote Avro schema is available in OpenShift Service Registry. If the Quote schema is not found, the producer publishes the schema to the registry. The producer then serializes the outgoing data using the schema and includes the registered schema ID in the message value.&lt;/p&gt; &lt;p&gt;A downstream consumer application can use the schema ID found in the message payload to fetch the necessary schema from the registry. The consumer application can then use the schema to validate and deserialize the incoming message.&lt;/p&gt; &lt;p&gt;To confirm that the Avro schema was published to OpenShift Service Registry:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to your &lt;a href="https://console.redhat.com/beta/application-services/service-registry"&gt;OpenShift Service Registry Instances&lt;/a&gt; listing.&lt;/li&gt; &lt;li&gt;Select the instance used by your producer application.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;Artifacts&lt;/strong&gt; tab.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You should see the Quote schema, as shown in Figure 4.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/quote_schema.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/quote_schema.png?itok=6Gj9pQGo" width="1440" height="810" alt="The Quote Avro schema is listed in the OpenShift Service Registry." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The Quote Avro schema listed in OpenShift Service Registry. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Select the Quote schema in the list and view the &lt;strong&gt;Content&lt;/strong&gt; tab. Use the &lt;strong&gt;Format&lt;/strong&gt; button to improve the legibility of the JSON, and confirm that it matches the &lt;code&gt;Quote.avsc&lt;/code&gt; file in the producer application codebase.&lt;/p&gt; &lt;h2&gt;Consuming messages in Service Registry&lt;/h2&gt; &lt;p&gt;The repository you cloned as part of this exercise contains a consumer application. This consumer application is configured using the same environment variables as the producer and reads messages from the &lt;code&gt;quote-requests&lt;/code&gt; topic. Because the producer and consumer use OpenShift Service Registry, the consumer can fetch the necessary Avro schema to validate and deserialize incoming quote requests.&lt;/p&gt; &lt;p&gt;Run the producer and consumer at the same time. Use cURL to open a connection to the server-sent events (SSE) endpoint at &lt;a href="http://localhost:8080/quotes"&gt;http://localhost:8080/quotes&lt;/a&gt;, then use another HTTP client to POST to &lt;a href="http://localhost:8080/quotes/request"&gt;http://localhost:8080/quotes/request&lt;/a&gt;. The consumer should correctly deserialize and process the items from the &lt;code&gt;quote-requests&lt;/code&gt; topic and place the processed quote into the &lt;code&gt;quotes&lt;/code&gt; topic, after which the SSE endpoint should display the items as shown in Figure 5.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/curl.jpeg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/curl.jpeg?itok=FEaLwNB0" width="1440" height="796" alt="A cURL command displays deserialized, JSON-formatted data received by the consumer at the SSE endpoint." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Deserialized, JSON-formatted data received by the consumer at the SSE endpoint and displayed by a cURL command. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;Enforcing schema compatibility rules&lt;/h2&gt; &lt;p&gt;OpenShift Service Registry supports various schema compatibility rules to prevent the publication of schema changes that could lead to incompatibilities with downstream applications (that is, breaking changes). You can read more about compatibility rules in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817"&gt;service documentation&lt;/a&gt;. To enable this enforcement:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open the Service Registry UI at &lt;a href="https://console.redhat.com/beta/application-services/service-registry"&gt;console.redhat.com/beta/application-services/service-registry&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Select your instance and view the Quote schema.&lt;/li&gt; &lt;li&gt;Set the &lt;strong&gt;Validity Rule&lt;/strong&gt; to &lt;strong&gt;Full&lt;/strong&gt; and the &lt;strong&gt;Compatibility Rule&lt;/strong&gt; to &lt;strong&gt;Backward&lt;/strong&gt; (see Figure 6).&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Upload new version&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;Paste in the following Avro schema and click &lt;strong&gt;Upload&lt;/strong&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt; &lt;code&gt;{ "namespace": "org.acme.kafka.quarkus", "type": "record", "name": "Quote", "fields": [ { "name": "id", "type": "string" }, { "name": "price", "type": "int" }, { "name": "notes", "type": "string" } ] }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Figure 6 shows these updates in the console.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rule.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/rule.png?itok=j6E6m_0t" width="1440" height="817" alt="The Compatibility Rule can be set in the OpenShift Service Registry UI." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Enforcing schema compatibility rules using the OpenShift Service Registry UI. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;An &lt;strong&gt;Invalid Content&lt;/strong&gt; error should be displayed, because this new schema violated the backward compatibility rule by adding a new required field. New fields must be optional if backward compatibility is enabled. As the error message indicated, new schemas are required to provide backward-compatible schemas for all future evolution.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Congratulations—in this article you have learned how to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use OpenShift Service Registry.&lt;/li&gt; &lt;li&gt;Use OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;li&gt;Create Avro schemas.&lt;/li&gt; &lt;li&gt;Integrate Java applications that use Avro schemas with both services.&lt;/li&gt; &lt;li&gt;Manage schema evolution and apply rules to prevent breaking changes for downstream consumers.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Sign up for the services described in this article, and let us know your experience in the comments.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/11/get-started-openshift-service-registry" title="Get started with OpenShift Service Registry"&gt;Get started with OpenShift Service Registry&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Evan Shortiss</dc:creator><dc:date>2022-07-20T16:00:00Z</dc:date></entry><entry><title type="html">An improved &amp;#8216;Spreadsheet like&amp;#8217; experience on DMN Editor</title><link rel="alternate" href="https://blog.kie.org/2022/07/a-better-spreadsheet-like-experience.html" /><author><name>Fabrizio Antonangeli</name></author><id>https://blog.kie.org/2022/07/a-better-spreadsheet-like-experience.html</id><updated>2022-07-20T14:33:15Z</updated><content type="html">The boxed expression editor is a key component of the DMN Editor. In previous articles, we introduced the new implementation of this . In this article, I’ll show how I extended the component, implementing the keyboard navigation for faster DMN editing. REQUIREMENTS * (1.46.0+); * (0.20.0+); , there is a ready-to-use online version of the DMN editor to try the new functionality. THE NEW KEYBOARD NAVIGATION The editing of a decision in the DMN Editor was based on the mouse interaction requiring the user to continuously switch between keyboard and mouse, resulting in a time-consuming activity. As my first task on the project, I worked on implementing a user experience as much as possible similar to Google Spreadsheet. As a result, the user can edit an expression, cell by cell, seamlessly using only the keyboard. THE NEW KEYBOARD ACTIONS AVAILABLE IN “VIEW MODE” Navigation between cells is now available in any type of expression using the arrow keys to go UP, RIGHT, DOWN, and LEFT, in a natural way. Continuous navigation is available with TAB, to jump to the next cell, or SHIFT-TAB to jump to the previous cell. This with the exception that if you are at the end of the row, you jump to the first cell of the next row or the last cell of the previous row if you are on the first cell of the row. After you choose the cell you want to edit, you just start writing the content, and this way, you erase the already existing content, if there is any. In addition, if you just want to start editing from the end of the cell’s content, you just need to press ENTER and start typing your content. Differently from a normal Spreadsheet, we have particular cells with nested tables or cells that don’t have just text, and when you click on them, a pop-up with a few inputs will appear, used in different cases. In this last case, you can now open the pop-up by pressing ENTER, and then you jump between the inputs inside the form using TAB/SHIFT-TAB. ENTER/ESC will close the pop-up and save or cancel your changes. For the case of nested tables, for instance, a "Context expression" with a Decision Table inside, you can jump inside the nested table with ENTER key and come back to the parent table with ESC. THE NEW KEYBOARD ACTIONS AVAILABLE IN “EDIT MODE” When you finish editing a cell, and you want to apply your changes, using TAB/SHIFT-TAB you save and jump directly to the next or previous cell, or to the cell below with ENTER. On the contrary, you can press ESC to cancel your changes to the cell. A big change to the UX was the introduction of the "newline" in the Decision Table’s cells, using the CTRL-ENTER. This change impacted the logic of the "copy &amp;amp; paste" that was based on single-line cells. To achieve that, the new logic is based on the parser, and now you can just copy your data to or from a Spreadsheet. The implementation The work has been mainly focused on the package inside repository. The main obstacle to implementing all these functionalities was the communication between React custom and third-party components and highlighting the selected cell. The easiest way was possibly the use of the “contenteditable” attribute, but that required a full rewriting of the components for the table and cells. After an evaluation of 4 other solutions, we decided to listen to the keyboard events from the TD HTML element representing the cell, then show the highlight through the “:focus” CSS selector. This way managing the "onBlur" event or memorizing the selected cell is not needed. Instead, when the focus is on the input, which is actually a Monaco editor, the highlight needs to be on the component inside the cell, that has the state data. This is done by adding the CSS class "editable-cell–edit-mode" to the main tag of the component EditableCell. Then to ensure the stability of the component we use "Jest" and "@testing-library/react" to render components. For the E2E test, we use "Cypress", which currently doesn’t support TAB key simulation which we managed with the plugin. CONCLUSION Creating or modifying a Decision can be a time-consuming activity if you have a lot of data to add inside. Giving the user the ability to write that data quickly and in the way, he was used to, was very important to us. We also didn’t want the user to learn a new way for that, and we wanted to give the same experience as Google Spreadsheet or Excel. The post appeared first on .</content><dc:creator>Fabrizio Antonangeli</dc:creator></entry></feed>
